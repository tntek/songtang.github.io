<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>title</title>
    <link rel="stylesheet" href="./style..css">
</head>
<body>
    <div class="container">
        <div class="title">
            <h2>Source-Free Domain Adaptation with Frozen Multimodal Foundation Model</h2>
            <p>CVPR2024</p>
            <div class="nav">
                <a href="#">Song Tang<sup>1,2,3</sup></a>
                <a href="#">Wenxin Su<sup>1</sup></a>
                <a href="#">Mao Ye<sup>*4</sup></a>
                <a href="#">Xiatian Zhu<sup>*5</sup></a>
            </div>
            <div class="jieshao">
                <p><sup>1</sup>University of Shanghai for Science and Technology <sup>2</sup>UniversitÂ¨at Hamburg <sup>3</sup>ComOriginMat Inc <sup>4</sup>University of Electronic Science and Technology of China
             <sup>5</sup>University of Surrey</p>
            </div>
        </div>
        <div class="menu">
            <a href="https://arxiv.org/pdf/2311.16510">Paper</a>
            <a href="https://github.com/tntek/tntek.github.io/blob/main/proj/sfda/cvpr24/difo/file/10888_supp.pdf">Supplementary</a>
            <a href="https://github.com/tntek/tntek.github.io/blob/main/proj/sfda/cvpr24/difo/file/poster.png">Poster</a>
            <a href="https://github.com/tntek/source-free-domain-adaptation">PyTorch Code</a>
        </div>
        <div class="title">
            <p>Abstract</p>
            <div class="Abstract">
                <p>Source-Free Domain Adaptation (SFDA) aims to adapt a source model for a target domain, with only access to
unlabeled target training data and the source model pretrained on a supervised source domain. Relying on pseudo
labeling and/or auxiliary supervision, conventional methods are inevitably error-prone. To mitigate this limitation,
in this work we for the first time explore the potentials of off-the-shelf vision-language (ViL) multimodal models (e.g.,
CLIP) with rich whilst heterogeneous knowledge. We find that directly applying the ViL model to the target domain in a
zero-shot fashion is unsatisfactory, as it is not specialized for this particular task but largely generic. To make it task specific, we propose a novel Distilling multImodal Foundation
mOdel (DIFO) approach. Specifically, DIFO alternates between two steps during adaptation: (i) Customizing the
ViL model by maximizing the mutual information with the target model in a prompt learning manner, (ii) Distilling
the knowledge of this customized ViL model to the target model. For more fine-grained and reliable distillation, we
further introduce two effective regularization terms, namely most-likely category encouragement and predictive consistency. Extensive experiments show that DIFO significantly
outperforms the state-of-the-art alternatives.</p>
        <div class="image-container">
            <img src="file/poster.png" alt="Method">
        </div>
        
        <div class="citation">
            <p>Citation</p>
            <span>If you find our work helpful in your research, please cite our work:</span>
            <div>
                @InProceedings{Kundu_2021_ICCV,<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;title={Generalize then Adapt: Source-Free Domain Adaptive Semantic Segmentation},<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;author={Kundu, Jogendra Nath and Kulkarni, Akshay and Singh, Amit and Jampani, Varun and Babu, R. Venkatesh},<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;month={October},<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;year={2021},<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;pages={7046-7056}<br>
                }
            </div>
            <p>License</p>
            <span>This project is licenced under an<a style="color: rgb(30, 108, 147);font-weight: bold;" href="#">[MIT License]</a>.</span>
            <p>Contact</p>
            <span>If you have any queries, please get in touch via email : <a style="color: rgb(30, 108, 147);font-weight: bold;" href="#"> jogendranathkundu@gmail.com</a></span>
        </div>
    </div>
</body>
</html>
